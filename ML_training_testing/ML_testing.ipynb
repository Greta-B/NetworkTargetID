{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d025650-11c2-4d8d-ad59-79835b78cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(716)\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from random import sample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f70707f-bc16-48bf-954b-4bc565d637f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"DTGpred_results\"\n",
    "t_dir = os.path.join(out_dir, 'test_data')\n",
    "if not os.path.exists(t_dir):\n",
    "    os.makedirs(t_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0d33bb-e4e3-4aa3-942b-2444ba230e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test set\n",
    "positive_test = [line.rstrip('\\n') for line in open(os.path.join(out_dir, \"testing_class1.txt\"))]\n",
    "\n",
    "# negative test set\n",
    "negative_test = [line.rstrip('\\n') for line in open(os.path.join(out_dir, \"testing_class0.txt\"))]\n",
    "\n",
    "# selected dataset\n",
    "selected_dataset = pd.read_csv(os.path.join(out_dir, \"selected_dataset.tsv\"), delimiter=\"\\t\", index_col=0, low_memory=False)\n",
    "test_dataset = selected_dataset.loc[selected_dataset.index.isin(positive_test + negative_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f687cfd-d0b6-42c4-8895-c02cdd124a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "3530\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_test))\n",
    "print(len(negative_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311f9604-3c12-4ff7-b9f0-ed348a20ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for drawing negative examples chuncks from list of genes\n",
    "def negative_sample_draw(gene_list, l = len(positive_test), n=0):\n",
    "    \"\"\"get the nth chunck of negative examples\"\"\"\n",
    "    return(gene_list[n*l:n*l+l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63068d83-f32e-44e2-8737-e6c0a875ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for creating test set based on draw n. Note: last column is the label\n",
    "def test_set_n(n=0):\n",
    "    negative_examples = negative_sample_draw(negative_test, l=len(positive_test), n=n)\n",
    "    test_examples = positive_test + negative_examples\n",
    "\n",
    "    test_dataset_n = test_dataset.loc[test_dataset.index.isin(test_examples)].copy()\n",
    "    test_dataset_n['Targets'] = 0.0\n",
    "    for target in test_dataset_n.index.to_list():\n",
    "        if target in positive_test:\n",
    "            test_dataset_n.loc[target, 'Targets'] = 1.0\n",
    "    random.seed(4)\n",
    "    test_dataset_n = shuffle(test_dataset_n)\n",
    "\n",
    "    # Double-check that the test dataset does not contain labels\n",
    "    test_data = test_dataset_n.iloc[:, 0:-1]\n",
    "    for i in range(len(test_data.columns)):\n",
    "        data = abs(test_data.iloc[:, i])\n",
    "        if data.equals(test_dataset_n.iloc[:, -1]):\n",
    "            raise Exception(\"Chunk n:\", n, \"target labels match feature:\", i, test_data.columns[i], \"nFeatures: \", test_data.shape[1])\n",
    "\n",
    "    # Export test dataset\n",
    "    test_data.to_csv(os.path.join(t_dir, f'test_data_n{n}.csv'), sep=\",\", index=True, header=True)\n",
    "    return(test_dataset_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7728f1-8837-49d9-b2b1-f1f78882e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for averaging prediction probabilities\n",
    "def averaging_predictions(results):\n",
    "    DTG_pr = []\n",
    "    for prediction in results['gene'].unique():\n",
    "        DTG_pr.append(\n",
    "            {\n",
    "                'Gene': prediction,\n",
    "                'Avg_probability': results[results['gene'] == prediction][1].mean()\n",
    "            }\n",
    "        )  \n",
    "    return(pd.DataFrame(DTG_pr).sort_values(by='Avg_probability', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d1a136-a33a-4def-8978-e7f3a8531e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to evaluate model m range r on test set n\n",
    "def model_performance_test(m = \"Ensemble\", r = 2, n = 0):\n",
    "    \n",
    "    # Create test data (X_test) and labels (y_test)\n",
    "    test_set = test_set_n(n=n)\n",
    "    gene_names = np.array(test_set.index.to_list())\n",
    "    X_test = test_set.iloc[:, 0:-1].values\n",
    "    y_test = test_set.iloc[:, -1].values\n",
    "\n",
    "    \n",
    "    n_predictions = {}\n",
    "    model_run = namedtuple(\"model_run\", [\"model\", \"sample\"])\n",
    "\n",
    "    # Evaluate range r of models m \n",
    "    for i in range(r):\n",
    "        # load model\n",
    "        read_dir = os.path.join(out_dir, f'draw_{i}')\n",
    "        model = pickle.load(open(os.path.join(read_dir, f'{m}_m{i}.sav'), 'rb'))\n",
    "        \n",
    "        # predictions and perfomance metrics\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision= precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pred = model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, pred[:,1])\n",
    "        \n",
    "        # save to pd dataframe\n",
    "        test_predictions = pd.DataFrame(pred)\n",
    "        test_predictions['gene'] = gene_names\n",
    "        test_predictions['model'] = m\n",
    "        test_predictions['sample'] = i\n",
    "        test_predictions = test_predictions[['gene', 0, 1, 'model', 'sample']]\n",
    "        test_predictions['Accuracy'] = '{0:0.5f}'.format(accuracy)\n",
    "        test_predictions['Precision'] = '{0:0.5f}'.format(precision)\n",
    "        test_predictions['Recall'] = '{0:0.5f}'.format(recall)\n",
    "        test_predictions['F1'] = '{0:0.5f}'.format(f1)\n",
    "        test_predictions['ROC_AUC'] = '{0:0.5f}'.format(roc_auc)\n",
    "\n",
    "        n_predictions[model_run(m, i)] = test_predictions\n",
    "\n",
    "    # export results tables    \n",
    "    df_predictions = pd.concat(n_predictions.values(), sort=False, join='outer', axis=0, ignore_index=True)\n",
    "    df_predictions.to_csv(os.path.join(t_dir, f'{m}_test_n{n}_full.tsv'), index=False, sep=\"\\t\")\n",
    "\n",
    "    predictions = averaging_predictions(df_predictions)\n",
    "    predictions.to_csv(os.path.join(t_dir, f'{m}_test_n{n}_avg.tsv'), index=False, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a6d730-365f-4e2b-9595-04c0fe942c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(89):\n",
    "    model_performance_test(m = \"Ensemble\", r = 89, n = t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e3fe8-fa54-47ad-93a0-d248b7735596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensembl_ML",
   "language": "python",
   "name": "ensembl_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
